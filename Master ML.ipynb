{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv\n",
    "data = pd.read_csv('dataset/advertising.csv')\n",
    "data = pd.read_csv('', na_values=[\" ?\"])\n",
    "data = pd.read_csv('', skiprows= 5)\n",
    "data = pd.read_csv('', delimiter='\\t')\n",
    "data = pd.read_csv('', nrows=100)\n",
    "data = pd.read_csv('',  usecols= ['Item_Identifier','Item_Type',])\n",
    "\n",
    "#excel\n",
    "sheet_1985 = pd.read_excel('dataset/sales.xlsx',sheet_name='1985')\n",
    "sheet_1985 = pd.read_excel('',sheet_name='1985')\n",
    "\n",
    "#json\n",
    "data = pd.read_json('datasets/simple.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(), data.describe().round(2),  data.describe().T, \n",
    "data.info(), data.isnull().sum(), data.isnull().sum().sort_index().head()\n",
    "data.shape, data.columns, data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index('Item_Identifier',inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.Year == 1987]\n",
    "data.loc[(data.Year == 2009) & (data.Size == 'Medium')]\n",
    "data[(data.Year == 1987) | (data.Year == 1988) | (data.Year == 1999)]\n",
    "\n",
    "# list of columns\n",
    "select_columns = ['Item_Identifier', 'Item_MRP', 'Outlet_Establishment_Year', 'Outlet_Size']\n",
    "data[select_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping (dictionary) to update values\n",
    "mapping = {\n",
    "    'Low Fat' : 'LF',\n",
    "    'Regular' : 'R',\n",
    "    'LF' : 'LF',\n",
    "    'reg': 'R',\n",
    "    'low fat' : 'LF'\n",
    "}\n",
    "data.Item_Fat_Content.map(mapping)\n",
    "data.Item_Fat_Content = data.Item_Fat_Content.map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide MRP by 74\n",
    "data.Item_MRP.apply(lambda x: x/74)\n",
    "\n",
    "def convert(price):\n",
    "    price = price/74\n",
    "    price = price + 1.28\n",
    "    return price\n",
    "data['MRP_USD'] = data.Item_MRP.apply(lambda x : convert(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort values\n",
    "data_frame.sort_values(by=['grade'])\n",
    "data_frame.sort_values(by=['grade','marks'],ascending=[True,False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing data or use simple imputer used below\n",
    "data = data.fillna(data.mean())\n",
    "data['PRCP'] = data['PRCP'].fillna(data['PRCP'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Ad Topic Line', 'City', 'Country', 'Timestamp'], axis=1)\n",
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new columns from existing columns\n",
    "data['Total_Marks'] = data['math score']+data['reading score']+data['writing score']\n",
    "data['Percentage'] = data['Total_Marks']/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer function to fill null values\n",
    "def impute_age(cols):\n",
    "    Age = cols[0]\n",
    "    Pclass = cols[1]\n",
    "    if pd.isnull(Age):\n",
    "        if Pclass == 1:\n",
    "            return 37\n",
    "        elif Pclass == 2:\n",
    "            return 30\n",
    "        else:\n",
    "            return 25\n",
    "    else:\n",
    "        return Age\n",
    "data['Age'] = data[['Age','Pclass']].apply(impute_age, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype({\"EmployeeName\": 'string', \"JobTitle\": 'category'})\n",
    "data['Snowfall']=pd.to_numeric(data['Snowfall'],errors='coerce')\n",
    "data['Date']=pd.to_datetime(data['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Class'] == 0].Amount.describe()\n",
    "data[data['Class'] == 1].Amount.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Job'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['SalStat']=data['SalStat'].map({' less than or equal to 50,000':0, ' greater than 50,000':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Email'].apply(lambda x : x.split('@')[1]).value_counts().head()\n",
    "data['CC Exp Date'].apply(lambda x : x.split('/')[1]).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['figure.figsize']=(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,10)) # size of graph\n",
    "sns.heatmap(data.corr(), cmap='RdBu', annot=True, fmt=\".2f\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"Item_Weight\", y=\"Item_MRP\",data=data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Clicked on Ad', data=data, palette=\"Set1\", hue='Male').set(title=\"Gender based Clicking on Ad\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"Item_Type\", y=\"Item_MRP\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(y='BasePay', col='Year', data=data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"Outlet_Size\", y=\"Item_Outlet_Sales\", kind='swarm',data=data);\n",
    "#kind='box', 'violin', 'boxen', 'point', 'bar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='Age',y='Daily Time Spent on Site',data=data, hue=\"Clicked on Ad\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Area Income',y='Daily Time Spent on Site', hue='Clicked on Ad', data=data, palette='rocket');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='SalStat', y='age', data=data).set(title=\"Receiving salary based on Age\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='fico',data= data, kde=True,bins = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='Length of Membership',y='Yearly Amount Spent',data=data).set(title=\"Effect of Length of Membership\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data[\"Purchase Price\"], kde=False, bins=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, hue='Clicked on Ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(2,2,1)\n",
    "sns.violinplot(x='Species',y='PetalLengthCm',data=data)\n",
    "plt.subplot(2,2,2)\n",
    "sns.violinplot(x='Species',y='PetalWidthCm',data=data)\n",
    "plt.subplot(2,2,3)\n",
    "sns.violinplot(x='Species',y='SepalLengthCm',data=data)\n",
    "plt.subplot(2,2,4)\n",
    "sns.violinplot(x='Species',y='SepalWidthCm',data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.subplots_adjust(top=1,bottom=0,right=1,left=0,hspace=0.25,wspace=0.5)\n",
    "plt.subplot(131)\n",
    "plt.title(\"Math score v/s Gender\")\n",
    "sns.barplot(x=\"gender\",y='math score',data=data)\n",
    "plt.subplot(132)\n",
    "plt.title(\"Reading Score V/S Gender\")\n",
    "sns.barplot(x='gender',y='reading score',data=data)\n",
    "plt.subplot(133)\n",
    "plt.title(\"Writing Score V/S Gender\")\n",
    "sns.barplot(x='gender',y='writing score',data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple imputer function to fill missing data\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding independent variables\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))\n",
    "\n",
    "# encoding dependent variables\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "numeric_features = ['Daily Time Spent on Site', 'Age', 'Area Income','Daily Internet Usage']\n",
    "data[numeric_features] = scaler.fit_transform(data[numeric_features])\n",
    "# x_train = ss.fit_transform(x_train)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data[numeric_features] = scaler.fit_transform(data[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the visible outliers in scatterplot\n",
    "df1.drop(df1[(df1['MinTemp'] < -15) & (df1['MaxTemp'] > 15)].index, inplace = True)\n",
    "df1.drop(df1[(df1['MinTemp'] > 8) & (df1['MaxTemp'] < -15)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca-dimensionality reduction to reduce noise & standardize noise using eigenvalues\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(df)\n",
    "x_pca = pca.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda-dimensionality reduction & maximising separation in multi-classes\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components=2)\n",
    "X_train = lda.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "drop_list = ['perimeter_mean','radius_mean','compactness_mean','area_worst']\n",
    "x= x.drop(drop_list,axis=1)\n",
    "\n",
    "X = data.drop(['Clicked on Ad'], axis=1)\n",
    "y = data['Clicked on Ad']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing The Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works on any size of dataset, gives informations about relevance of features\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(x_train, y_train)\n",
    "predictions = lm.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works very well on non linear problems\n",
    "# need to choose the right polynomial degree for a good bias/variance tradeoff\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_reg = PolynomialFeatures(degree=4)\n",
    "X_poly = poly_reg.fit_transform(X)\n",
    "lin_reg_ = LinearRegression()\n",
    "lin_reg_.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easily adaptable, works very well on non-linear problems, not biased by outliers\n",
    "# compulsory to apply feature scaling\n",
    "from sklearn.svm import SVR\n",
    "regressor = SVR(kernel = 'rbf')\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works on both linear / nonlinear problems\n",
    "# overfitting can easily occur\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state = 0)\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# powerful and accurate\n",
    "# overfitting can easily occur, need to choose the number of trees\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators=10, random_state=0)\n",
    "model_rf.fit(X_train, Y_train)\n",
    "regressor.predict([[6.5]])\n",
    "predictions = model_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilistic approach, gives informations about statistical significance of features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression(random_state=0)\n",
    "logmodel.fit(X_train,y_train)\n",
    "predictions = logmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to choose the number of neighbours k\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "knn.fit(X_train,y_train)\n",
    "pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high performance on nonlinear problems, not biased by outliers, not sensitive to overfitting\n",
    "# not good choice for large number of features\n",
    "from sklearn.svm import SVC\n",
    "svc_model = SVC(kernel='rbf', random_state=0)# kernel='linear'\n",
    "svc_model.fit(x_train,y_train)\n",
    "predictions = svc_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficient, not biased by outliers, works on nonlinear problems, probabilistic approach\n",
    "# based on the assumption that features have same statistical relevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpretability, no need for feature scaling, works on both linear / nonlinear problems\n",
    "# overfitting can easily occur\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "dtree.fit(x_train,y_train)\n",
    "predictions = dtree.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# powerful and accurate, good performance on many problems,\n",
    "# overfitting can easily occur, need to choose the number of trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=10, criterion='entropy', random_state=0)\n",
    "rf = rf.fit(x_train,y_train)\n",
    "predictions = rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose number of clusters using the elbow method\n",
    "from sklearn.cluster import KMeans\n",
    "#elbow mwthod\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title(''), plt.xlabel(''), plt.ylabel(''), plt.show()\n",
    "\n",
    "# choose optimum number of clusters\n",
    "kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)\n",
    "kmeans.fit_predict(df.drop('Private',axis=1))\n",
    "kmeans.cluster_centers_\n",
    "kmeans.labels_\n",
    "\n",
    "# visualize clsuters\n",
    "plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n",
    "plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
    "plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n",
    "plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\n",
    "plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')\n",
    "plt.title(''), plt.xlabel(''), plt.ylabel(''), plt.legend(), plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding optimal number of clusters using dendograms\n",
    "import scipy.cluster.hierarchy as sch\n",
    "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\n",
    "plt.title(''), plt.xlabel(''), plt.ylabel(''), plt.show()\n",
    "\n",
    "# training the model\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hc = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\n",
    "y_hc = hc.fit_predict(X)\n",
    "\n",
    "# visualize clusters\n",
    "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Cluster 1')\n",
    "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')\n",
    "plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Cluster 3')\n",
    "plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\n",
    "plt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\n",
    "plt.title(''), plt.xlabel(''), plt.ylabel(''), plt.legend(), plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification models\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(classification_report(y_test,predictions))\n",
    "confusion_matrix(predictions, y_test)\n",
    "accuracy_score(predictions, y_test)\n",
    "# never place complete judgement on accuracy, focus on balance of accuracy, f1, and precison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression models\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(Y_test, y_pred_rf))\n",
    "print(model_rf.score(X_test, Y_test))\n",
    "rmse = (np.sqrt(mean_squared_error(y_test, predictions)))\n",
    "print(\"RMSE is {}\", format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results on Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, color = 'red')\n",
    "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
    "plt.title(''), plt.xlabel(''), plt.ylabel(''), plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = sc.inverse_transform(X_train), y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\n",
    "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title(''), plt.xlabel(''), plt.ylabel(''), plt.legend(), plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['price_range'] = mod.predict(test)\n",
    "test.head()\n",
    "test['Survived']=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = test[['id', 'price_range']].copy()\n",
    "final.to_csv('mobile_prices.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
